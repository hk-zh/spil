
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="imitation learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data </title>
  <link rel="icon" type="image/x-icon" href="static/images/capybara.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Language-Conditioned Imitation Learning with Base Skill Priors
              under Unstructured Data </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://hk-zh.github.io/Homepage/" target="_blank">Hongkuan Zhou</a>,
              </span>
              <span class="author-block">
                <a href="https://www.ce.cit.tum.de/air/people/zhenshan-bing-drrernat/" target="_blank">Zhenshan
                  Bing</a><sup>&#8224;</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.ce.cit.tum.de/air/people/xiangtong-yao/" target="_blank">Xiangtong Yao</a>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Xiaojie Su</a>
              </span>
              <span class="author-block">
                <a href="" target="_blank">Chenguang Yang</a>
              </span>
              <span class="author-block">
                <a href="" target="_blank">Kai Huang</a>
              </span>
              <span class="author-block">
                <a href="https://www.ce.cit.tum.de/air/people/prof-dr-ing-habil-alois-knoll/" target="_blank">Alios
                  Knoll</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Technical University of Munich</span>
              <span class="eql-cntrb"><small><br> <sup>&#8224;</sup>Corresponding Author</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2305.19075.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!--                      <span class="link-block">-->
                <!--                        <a href="static/pdfs/supplementary_material.pdf" target="_blank"-->
                <!--                        class="external-link button is-normal is-rounded is-dark">-->
                <!--                        <span class="icon">-->
                <!--                          <i class="fas fa-file-pdf"></i>-->
                <!--                        </span>-->
                <!--                        <span>Supplementary</span>-->
                <!--                        </a>-->
                <!--                      </span>-->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/hk-zh/spil" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2305.19075" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="static/videos/Single%20Task%20Completion.mp4" type="video/mp4">
        </video>

      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The growing interest in language-conditioned robot manipulation aims to develop robots capable of
              understanding and executing complex tasks, with the objective of enabling robots to interpret language
              commands and manipulate objects accordingly. While language-conditioned approaches demonstrate impressive
              capabilities for addressing tasks in familiar environments, they encounter limitations in adapting to
              unfamiliar environment settings. In this study, we propose a general-purpose, language-conditioned
              approach that combines base skill priors and imitation learning under unstructured data to enhance the
              algorithm's generalization in adapting to unfamiliar environments. We assess our model's performance in
              both simulated and real-world environments using a zero-shot setting. In the simulated environment, the
              proposed approach surpasses previously reported scores for CALVIN benchmark, especially in the challenging
              Zero-Shot Multi-Environment setting. The average completed task length, indicating the average number of
              tasks the agent can continuously complete, improves more than 2.5 times compared to the state-of-the-art
              method HULC. In addition, we conduct a zero-shot evaluation of our policy in a real-world setting,
              following training exclusively in simulated environments without additional specific adaptations. In this
              evaluation, we set up ten tasks and achieved an average 30\% improvement in our approach compared to the
              current state-of-the-art approach, demonstrating a high generalization capability in both simulated
              environments and the real world.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- Image carousel -->
  <!-- <section class="hero is-small"> -->
  <!-- <div class="hero-body"> -->
  <!-- <div class="container is-max-desktop content"> -->
  <!-- <div class="carousel results-carousel"> -->

  <!-- <div class="item"> -->
  <!-- Your image here -->
  <!-- <img src="static/images/skill_space.png" alt="MY ALT TEXT" /> -->
  <!-- <div class="content has-text-justified"> -->
  <!-- <p> -->
  <!-- This architecture comprises two encoders - the skill prior encoder and the action sequence encoder, and
                a decoder for reconstructing the skill embeddings into action sequences. The skill prior encoder takes
                one-hot-key embeddings of translation, rotation, and grasping as input and outputs the distribution of
                the base skill prior in the skill latent space. The action sequence encoder encodes the action sequences
                with a fixed horizon of &hamilt; to the distribution of skill in the latent space. The decoder then
                reconstructs the skill embedding into action sequences. -->
  <!-- </p> -->
  <!-- </div> -->
  <!-- </div> -->
  <!-- <div class="item"> -->
  <!-- Your image here -->
  <!-- <figure class="image"> -->
  <!-- <img src="static/images/architecture.png" alt="MY ALT TEXT" /> <br> -->
  <!-- <div class="content has-text-justified"> -->
  <!-- <p> -->
  <!-- The architecture of the proposed method. Following the encoding process, the static observation,
                  gripper observation, and language instruction are generated to embeddings for the plan, language goal,
                  language, static observation, and gripper observation. The skill selector module subsequently decodes
                  a sequence of skill embeddings using the plan, observation, and language goal embeddings. The skill
                  labeler is responsible for labelling the skill embeddings with the base skills: translation, rotation,
                  and grasping. The base skill regularization loss is calculated based on the base skill prior
                  distributions (from base skill locator &#8492;), selected skill instance, and labelled probability
                  indicating its belonging to specific base skills. This labelled probability is also leveraged to
                  determine the categorical regularization loss. Finally, the pre-trained and frozen skill generator
                  &Gscr; decodes all the skill embeddings into action sequences, which are then utilized to calculate
                  the reconstruction loss (Huber loss). -->
  <!-- </p> -->
  <!-- </div> -->
  <!-- </figure> -->
  <!-- </div> -->
  <!-- </div> -->
  <!-- </div> -->
  <!-- </div> -->
  <!-- </section> -->
  <!-- End image carousel -->




  <!-- Youtube video -->
  <!--<section class="hero is-small is-light">-->
  <!--  <div class="hero-body">-->
  <!--    <div class="container">-->
  <!--      &lt;!&ndash; Paper video. &ndash;&gt;-->
  <!--      <h2 class="title is-3">Video Presentation</h2>-->
  <!--      <div class="columns is-centered has-text-centered">-->
  <!--        <div class="column is-four-fifths">-->
  <!--          -->
  <!--          <div class="publication-video">-->
  <!--            &lt;!&ndash; Youtube embed code here &ndash;&gt;-->
  <!--            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
  <!--          </div>-->
  <!--        </div>-->
  <!--      </div>-->
  <!--    </div>-->
  <!--  </div>-->
  <!--</section>-->
  <!-- End youtube video -->


  <!-- Video carousel -->
  <!--<section class="hero is-small is-light">-->
  <!--  <div class="hero-body">-->
  <!--    <div class="container is-max-desktop content">-->
  <!--      <h2 class="title is-3">Videos</h2>-->
  <!--      <div id="results-carousel" class="carousel results-carousel">-->
  <!--        <div class="item item-video1">-->
  <!--          <video poster="" id="video1" autoplay controls muted loop height="100%">-->
  <!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
  <!--            <source src="static/videos/videoplayback.mp4"-->
  <!--            type="video/mp4">-->
  <!--          </video>-->
  <!--        </div>-->
  <!--        <div class="item item-video2">-->
  <!--          <video poster="" id="video2" autoplay controls muted loop height="100%">-->
  <!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
  <!--            <source src="static/videos/Sequential%20Task%20Completion.mp4"-->
  <!--            type="video/mp4">-->
  <!--          </video>-->
  <!--        </div>-->
  <!--      </div>-->
  <!--    </div>-->
  <!--  </div>-->
  <!--</section>-->
  <!-- End video carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop content">
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <h1 id="appendix">Appendix</h1>
            <h2 id="terminology">Terminology</h2>
            <div id="tab:terminology">
              <table>
                <caption><b>Table 1: Terminology</b></caption>
                <thead>
                  <tr class="header">
                    <th style="text-align: left;">Notations</th>
                    <th style="text-align: left;">Definition</th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="odd">
                    <td style="text-align: left;"><span class="math inline">\(\mathcal{S}\)</span></td>
                    <td style="text-align: left;">State space</td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: left;"><span class="math inline">\(\mathcal{A}\)</span></td>
                    <td style="text-align: left;">Action space</td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: left;"><span class="math inline">\(\mathcal{A}_\textrm{skill}\)</span></td>
                    <td style="text-align: left;">Skill space</td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: left;"><span class="math inline">\(\mathcal{A}_z\)</span></td>
                    <td style="text-align: left;">Skill embedding space</td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: left;"><span class="math inline">\(\mathcal{P}\)</span></td>
                    <td style="text-align: left;">Environment dynamics</td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: left;"><span class="math inline">\(\mathcal{G}\)</span></td>
                    <td style="text-align: left;">Multi-context goal space</td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: left;"><span class="math inline">\(\mathcal{I}\)</span></td>
                    <td style="text-align: left;">Language instruction set</td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: left;"><span class="math inline">\(x\)</span></td>
                    <td style="text-align: left;">Action sequence <span class="math inline">\((a_0, a_1, ...)\)</span>
                    </td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: left;"><span class="math inline">\(y\)</span></td>
                    <td style="text-align: left;">Base skills, <span class="math inline">\(y
                        \in \{\textrm{translation}, \textrm{rotation},
                        \textrm{grasping}\}\)</span></td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: left;"><span class="math inline">\(z\)</span></td>
                    <td style="text-align: left;">Skill embedding in the latent space</td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: left;"><span class="math inline">\(N_h\)</span></td>
                    <td style="text-align: left;">Horizon of action sequence (skill)</td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: left;"><span class="math inline">\(N_o\)</span></td>
                    <td style="text-align: left;">Number of observations</td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: left;"><span class="math inline">\(N_z\)</span></td>
                    <td style="text-align: left;">Skill embedding dimension</td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: left;"><span class="math inline">\(f_{\boldsymbol{\kappa}}\)</span></td>
                    <td style="text-align: left;">Base skill locator network with parameters
                      <span class="math inline">\(\boldsymbol{\kappa}\)</span>
                    </td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: left;"><span class="math inline">\(f_{\boldsymbol{\theta}}\)</span></td>
                    <td style="text-align: left;">Skill generator network with parameters
                      <span class="math inline">\(\boldsymbol{\theta}\)</span>
                    </td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: left;"><span class="math inline">\(f_{\boldsymbol{\phi}}\)</span></td>
                    <td style="text-align: left;">Encoder network for action sequences</td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: left;"><span class="math inline">\(f_{\boldsymbol{\Phi}}\)</span></td>
                    <td style="text-align: left;">Encoder network of our SPIL model</td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: left;"><span class="math inline">\(f_{\boldsymbol{\lambda}}\)</span></td>
                    <td style="text-align: left;">Skill embedding selector network with
                      parameters <span class="math inline">\(\boldsymbol{\lambda}\)</span></td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: left;"><span class="math inline">\(f_{\boldsymbol{\omega}}\)</span></td>
                    <td style="text-align: left;">Base skill selector network with
                      parameters <span class="math inline">\(\boldsymbol{\omega}\)</span></td>
                  </tr>
                </tbody>
              </table>
            </div>
            <p><span id="tab:terminology" label="tab:terminology"></span></p>
            <h2 id="sec:problem statement">Problem Statement</h2>
            <p>Our methods aim to learn a goal-conditioned policy <span class="math inline">\(\pi(a|s,l)\)</span> that
              outputs
              action <span class="math inline">\(a \in \mathcal{A}\)</span>, conditioned on the
              current state <span class="math inline">\(s \in \mathcal{S}\)</span> and
              a language instruction <span class="math inline">\(l \in
                \mathcal{I}\)</span>, under environment dynamics <span class="math inline">\(\mathcal{P}: \mathcal{S}
                \times
                \mathcal{A}
                \rightarrow \mathcal{S}\)</span>. The environment can be characterized
              by the following statements:</p>
            <ul>
              <li>
                <p>A multidimensional action space <span class="math inline">\(\mathcal{A} \subset
                    \mathbb{R}^7\)</span>.
                  This
                  actions space contains all parameters to drive the agent to finish
                  tasks. The first three parameters are the displacement of the
                  end-effector’s position. Another three parameters are the rotation of
                  the end effector and the final parameters is the gripper control
                  parameters.</p>
              </li>
              <li>
                <p>A visual state <span class="math inline">\(\mathcal{S} \subset
                    \mathbb{R}^{N_o \times H \times W \times 3}\)</span>, where <span class="math inline">\(N_o\)</span>
                  is the
                  number of observations. H and
                  W are the height and width of the images. 3 is the channel numbers since
                  the agent only has access to visual observations from cameras.</p>
              </li>
              <li>
                <p>A multi-context goal space consists of language instructions and
                  goal images <span class="math inline">\(\mathcal{G} \subset \mathcal{I}
                    \cup \mathbb{R}^{H \times W \times 3}\)</span>, where <span
                    class="math inline">\(\mathcal{I}\)</span>
                  is the
                  nature language set
                  and <span class="math inline">\(H\)</span>, <span class="math inline">\(W\)</span> are the height and
                  width of
                  images,
                  respectively.</p>
              </li>
            </ul>
            <h2 id="sec:CALVIN Environment Detail">CALVIN Environment Detail</h2>
            <p>Mees et al. <a href="#ref:calvin" data-reference-type="ref"
              data-reference="ref:calvin">[1]</a> introduce the CALVIN benchmark to facilitate
              learning language-conditioned tasks across four manipulation
              environments. We mainly use this benchmark to evaluate our SPIL model’s
              performance. CALVIN benchmark mainly contains three components.</p>
            <ul>
              <li>
                <p><strong>CALVIN environments</strong>. CALVIN includes four
                  distinct environments (A, B, C, D) that are interconnected in terms of
                  their underlying structure. Each environment consists of one Franka
                  Emika Panda robot arm equipped with a gripper and a desk featuring a
                  sliding door and a drawer that can be opened and closed. On the desk,
                  there exists a button that can toggle the green light and a switch to
                  control a light bulb. Note that each environment has a different desk
                  with various of textures and the position of static elements such as the
                  sliding door, drawer, light, switch, and button are different across
                  each environment.</p>
              </li>
              <li>
                <p><strong>CALVIN dataset</strong>. To comprehensively explore the
                  possible scenarios within a given space, the individuals involved
                  engaged in teleoperated play while wearing an HTC Vive VR headset for a
                  total of 24 hours, spending roughly the same amount of time (6 hours) in
                  each of four different environments. In terms of language instructions,
                  they utilize 400 natural language instructions that correspond to over
                  34 different tasks to label episodes in a procedural manner, based on
                  the recorded state of the environment in the CALVIN dataset.</p>
              </li>
              <li>
                <p><strong>CALVIN challenge</strong>. The authors of the CALVIN
                  introduce various evaluation protocols and metrics of different
                  difficulty levels. These protocols are</p>
                <ul>
                  <li>
                    <p><strong>Single Environment:</strong> Training in a single
                      environment and evaluating the policy in the same environment.</p>
                  </li>
                  <li>
                    <p><strong>Multi Environment:</strong> Training in all four
                      environments and evaluating the policy in one of them.</p>
                  </li>
                  <li>
                    <p><strong>Zero-Shot Multi Environment:</strong> This involves
                      training the agent in three different environments and then testing its
                      ability to generalize and perform well in a fourth environment that it
                      has not previously encountered.</p>
                  </li>
                </ul>
                <p>and the metrics are</p>
                <ul>
                  <li>
                    <p><strong>Multi-Task Language Control (MTLC):</strong> The most
                      straightforward evaluation aims to verify how well the learned
                      multi-task language-conditioned policy generalizes 34 manipulation
                      tasks</p>
                  </li>
                  <li>
                    <p><strong>Long-Horizon Multi-Task Language Control
                        (LH-MTLC):</strong> In this evaluation, the 34 tasks from a previous
                      evaluation are treated as subgoals, and valid sequences consisting of
                      five sequential tasks are computed.</p>
                  </li>
                </ul>
              </li>
            </ul>
            <h2 id="sec:Comparison with Other Skill-based Approaches">Comparison
              with Other Skill-based Approaches</h2>
            <p>To showcase our model’s exceptional performance relative to other
              skill-based reinforcement learning approaches, we’ve adapted the CALVIN
              benchmark to match the assessment criteria utilized in those approaches.
              This modified benchmark focuses on a subset of tasks within the CALVIN
              benchmark for evaluation purposes. The baselines we choose are two
              skill-based reinforcement learning approaches SpiRL <a href="#ref:SpiRL" data-reference-type="ref"
              data-reference="ref:SpiRL">[2]</a>
              and SkiMo <a href="#ref:SkiMo" data-reference-type="ref"
              data-reference="ref:SkiMo">[3]</a>. We assess
              these methods using a fixed task chain comprising four assignments,
              namely Open Drawer - Turn on Lightbulb - Move Slider Left - Turn on LED.
              This task sequence is evaluated 1000 times to determine the average
              success rate. The outcomes are presented in Table <a href="#tab:skill-based approach comparisons"
                data-reference-type="ref" data-reference="tab:skill-based approach comparisons">2</a>. Our SPIL
              approach consistently attains an almost perfect success rate of nearly
              100% in this task sequence, outperforming the other baseline methods
              that utilize skill-based reinforcement learning for agent training. All
              experiments are evaluated with 3 random seeds.
            </p>
            <div id="tab:skill-based approach comparisons">
              <table>
                <caption><b>Table 2: Skill-based approaches</b></caption>
                <thead>
                  <tr class="header">
                    <th style="text-align: left;">Model</th>
                    <th style="text-align: left;">SpiRL</th>
                    <th style="text-align: left;">SkiMO</th>
                    <th style="text-align: left;">SPIL(ours)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="odd">
                    <td style="text-align: left;">Avg. Len. (#/4.00)</td>
                    <td style="text-align: left;"><span class="math inline">\(3.02
                        (0.53)\)</span></td>
                    <td style="text-align: left;"><span class="math inline">\(3.64
                        (0.21)\)</span></td>
                    <td style="text-align: left;"><span class="math inline">\(\textbf{3.99}
                        (0.01)\)</span></td>
                  </tr>
                </tbody>
              </table>
            </div>
            <h2 id="implementation-details">Implementation Details</h2>
            <h3 id="skill-embedding-space-generation">Skill Embedding Space
              Generation</h3>
            <p>For the Single Environment, the skill embedding space is generated by
              the action sequences in the training data of environment D. Regarding to
              Zero-shot Multi Environment, the skill embedding space is generated by
              the action sequences in the training data of environment A, B, and C.
              Important hyperparameters are listed in Table <a
                href="#table: important hyperparameters (Single Environment)" data-reference-type="ref"
                data-reference="table: important hyperparameters (Single Environment)">3</a>
              and <a href="#table: important hyperparameters (Zero-shot Multi Environment)" data-reference-type="ref"
                data-reference="table: important hyperparameters (Zero-shot Multi Environment)">4</a>.</p>
            <h3 id="training-setting">Training Setting</h3>
            <p>The hyperparameters leveraged to train the agent in Single
              Environment and Zero-shot Multi Environment settings are listed in Table
              <a href="#table: important hyperparameters (Single Environment)" data-reference-type="ref"
                data-reference="table: important hyperparameters (Single Environment)">3</a>
              and <a href="#table: important hyperparameters (Zero-shot Multi Environment)" data-reference-type="ref"
                data-reference="table: important hyperparameters (Zero-shot Multi Environment)">4</a>.
              The camera observations are applied with an image augmentation strategy.
              For simulation evaluations, the static observation goes through random
              shift of 10 pixels and normalization with mean=[0.48145466, 0.4578275,
              0.40821073] and std=[0.26862954, 0.26130258, 0.27577711]; the gripper
              observation goes through random shift of 4 pixels and normalization with
              the same mean and std as static observation;
            </p>
            <p>For real-world experiments, we apply stronger augmentation for
              images. The static observation goes through the following transforms,
              random shift of 10 pixels, color jitter with 0.2 brightness and 0.2
              contrast, random rotation with the range of (-5, 5) degrees, random
              perspective with distortion-scale 0.1, and finally normalization with
              mean=[0.48145466, 0.4578275, 0.40821073] and std=[0.26862954,
              0.26130258, 0.27577711]. The gripper observation goes through center
              cropping, random shift with 4 pixels, color jitter with 0.2 brightness
              and 0.2 contrast, and finally the same normalization as the static
              observation.</p>
            <p>We have also implemented an augmentation strategy for action
              sequences during the skill embedding training, where we randomly set the
              last three relative actions of a sequence to zero, indicating still
              actions.</p>
            <div id="table: important hyperparameters (Single Environment)">
              <table>
                <caption><b>Table 3: Important hyperparameters (Single Environment)</b></caption>
                <thead>
                  <tr class="header">
                    <th style="text-align: center;">Description</th>
                    <th style="text-align: center;">Value</th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="odd">
                    <td style="text-align: center;">Batch Size</td>
                    <td style="text-align: center;">64</td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: center;">Learning Rate</td>
                    <td style="text-align: center;"><span class="math inline">\(1.0 \times
                        10^{-4}\)</span></td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: center;">Skill Embedding Dimension</td>
                    <td style="text-align: center;">20</td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: center;">Horizon Length <span class="math inline">\(H\)</span> of Skill</td>
                    <td style="text-align: center;">5</td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: center;">Magic Scales <span class="math inline">\(w_1, w_2, w_3\)</span></td>
                    <td style="text-align: center;">1.4, 3.0, 0.75</td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: center;">Plan prior matching weight <span
                        class="math inline">\(\beta\)</span>
                    </td>
                    <td style="text-align: center;"><span class="math inline">\(5.0 \times
                        10^{-4}\)</span></td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: center;">Regularizer weight <span class="math inline">\(\beta_1\)</span></td>
                    <td style="text-align: center;"><span class="math inline">\(1.0 \times
                        10^{-4}\)</span></td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: center;">Regularizer weight <span class="math inline">\(\beta_2\)</span></td>
                    <td style="text-align: center;"><span class="math inline">\(1.0 \times
                        10^{-5}\)</span></td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: center;">Regularizer weight <span class="math inline">\(\gamma_1\)</span>
                    </td>
                    <td style="text-align: center;"><span class="math inline">\(5.0 \times
                        10^{-3}\)</span></td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: center;">Regularizer weight <span class="math inline">\(\gamma_2\)</span>
                    </td>
                    <td style="text-align: center;"><span class="math inline">\(1.0 \times
                        10^{-5}\)</span></td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div id="table: important hyperparameters (Zero-shot Multi Environment)">
              <table>
                <caption><b>Table 4: Important hyperparameters (Zero-shot Multi
                    Environment)</b></caption>
                <thead>
                  <tr class="header">
                    <th style="text-align: center;">Description</th>
                    <th style="text-align: center;">Value</th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="odd">
                    <td style="text-align: center;">Batch Size</td>
                    <td style="text-align: center;">32</td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: center;">Learning Rate</td>
                    <td style="text-align: center;"><span class="math inline">\(1.0 \times
                        10^{-4}\)</span></td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: center;">Skill Embedding Dimension</td>
                    <td style="text-align: center;">20</td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: center;">Horizon Length <span class="math inline">\(H\)</span> of Skill</td>
                    <td style="text-align: center;">5</td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: center;">Magic Scales <span class="math inline">\(w_1, w_2, w_3\)</span></td>
                    <td style="text-align: center;">1.4, 3.0, 0.75</td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: center;">Plan prior matching weight <span
                        class="math inline">\(\beta\)</span>
                    </td>
                    <td style="text-align: center;"><span class="math inline">\(1.0 \times
                        10^{-4}\)</span></td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: center;">Regularizer weight <span class="math inline">\(\beta_1\)</span></td>
                    <td style="text-align: center;"><span class="math inline">\(1.0 \times
                        10^{-4}\)</span></td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: center;">Regularizer weight <span class="math inline">\(\beta_2\)</span></td>
                    <td style="text-align: center;"><span class="math inline">\(1.0 \times
                        10^{-5}\)</span></td>
                  </tr>
                  <tr class="odd">
                    <td style="text-align: center;">Regularizer weight <span class="math inline">\(\gamma_1\)</span>
                    </td>
                    <td style="text-align: center;"><span class="math inline">\(5.0 \times
                        10^{-3}\)</span></td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: center;">Regularizer weight <span class="math inline">\(\gamma_2\)</span>
                    </td>
                    <td style="text-align: center;"><span class="math inline">\(1.0 \times
                        10^{-5}\)</span></td>
                  </tr>
                </tbody>
              </table>
            </div>
            <h2 id="computation-time">Computation Time</h2>
            <div id="tab:computation time">
              <table>
                <caption><b>Table 5: Training time in hours</b></caption>
                <thead>
                  <tr class="header">
                    <th style="text-align: left;">Environment</th>
                    <th style="text-align: left;">LangLfP</th>
                    <th style="text-align: left;">HULC</th>
                    <th style="text-align: left;">SPIL(ours)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="odd">
                    <td style="text-align: left;">D <span class="math inline">\(\rightarrow\)</span> D</td>
                    <td style="text-align: left;">30</td>
                    <td style="text-align: left;">42</td>
                    <td style="text-align: left;">43</td>
                  </tr>
                  <tr class="even">
                    <td style="text-align: left;">ABC <span class="math inline">\(\rightarrow\)</span> D</td>
                    <td style="text-align: left;">102</td>
                    <td style="text-align: left;">122</td>
                    <td style="text-align: left;">125</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <p><strong>Hardware and Software</strong>: All of the experiments were
              performed on a virtual machine with 40 virtual processing units, 356 GB
              RAM, and two Tesla V100 (16GB) GPUs. The virtual machine is equipped
              with the Ubuntu-20.04-LTS-focal operating system. Table <a href="#tab:computation time"
                data-reference-type="ref" data-reference="tab:computation time">5</a> shows the training time for
              each model, which was done with 40 epochs for Single Environment (D
              <span class="math inline">\(\rightarrow\)</span> D) and 30 epochs for
              Zero-shot Multi Environment (ABC <span class="math inline">\(\rightarrow\)</span> D).
            </p>
            <h2 id="theoretical-motivation">Theoretical Motivation</h2>
            <h3 id="subsec:Continuous Skill Embeddings With Base Skill Priors">Continuous
              Skill Embeddings With Base Skill Priors</h3>
            <figure id="fig:example" style="width:70%">
              <img src="static/images/skill_latent_space.png" style="width:98.0%" />
              <figcaption>Figure 1: Skill Latent Space</figcaption>
            </figure>
            <p>We define <span class="math inline">\(y\)</span> as the indicator for
              base skills and the base skill distribution in the latent space can be
              written as <span class="math inline">\(z \sim p(z|y)\)</span>. For given
              action sequence <span class="math inline">\(x\)</span>, we employ the
              approximate variational posterior <span class="math inline">\(q(z|x)\)</span> and <span
                class="math inline">\(q(y,z|x)\)</span> to estimate the intractable true
              posterior. Following the VAEs procedure, we measure the Kullback-Leibler
              (KL) divergence between the true posterior and the posterior
              approximation to determine the ELBO:
            <div style="display: flex;align-items: center;">
              <span class="math display" style="width:90%" id="eq:kl_pq_1">\[\label{eq:kl_pq_1}
                \begin{split}
                D_{KL}(q(y,z|x) || p(y,z|x))&amp; = \int_y \int_z q(y,z|x) \log
                \frac{q(y,z|x)}{p(y,z|x)}dzdy \\
                &amp; = - \int_y \int_z q(y,z|x) \log
                \frac{p(y,z|x)}{q(y,z|x)}dzdy \\
                &amp; = - \int_y \int_z q(y,z|x) \log
                \frac{p(x,y,z)}{q(z,y|x)}dzdy\\
                &amp; + \log p(x) \\
                \end{split}\]
              </span>
              <div style="text-align: center">(1)</div>
            </div>
            </p>
            <p>
            <div style="display: flex;align-items: center;">
              <span class="math display" style="width:90%" id="eq:kl_pq_2">\[\label{eq:kl_pq_2}
                % \resizebox{0.48\textwidth}{!}{%
                % $
                \begin{split}
                D_{KL}(q(z|x)||p(z|x)) = - \int_z q(z|x) \log
                \frac{p(z,x)}{q_(z|x)} dz + \log p(x)
                \end{split}
                % $%
                % }\]
              </span>
              <div style="text-align: center">(2)</div>
            </div>
            By combining Equation <a href="#eq:kl_pq_1" data-reference-type="ref"
            data-reference="eq:kl_pq_1">(1)</a>
            and Equation <a href="#eq:kl_pq_2" data-reference-type="ref"
            data-reference="eq:kl_pq_2">(2)</a>, we have:
            <div style="display: flex;align-items: center;">
              <span style="width:90%">\[\label{eq:elbo+kl}
                \begin{split}
                \log p(x) &amp; = \frac{1}{2} \bigg( \overbrace{\int_y \int_z
                q(y,z|x) \log \frac{p(x,y,z)}{q(z,y|x)p(x)}dzdy}^{\mathcal{L}_1} \\
                &amp; + \overbrace{\int_z q(z|x) \log \frac{p(z,x)}{q(z|x)} dz}
                ^{\mathcal{L}_2} + D_{KL}(q(y,z|x) || p(y,z|x)) \\
                &amp; + D_{KL}(q(z|x)||p(z|x))\bigg)
                \end{split}\]
              </span>
              <div style="text-align: center">(3)</div>
            </div>
            We focus on the ELBO term <span class="math inline">\(\mathcal{L}_{\textrm {ELBO}} =
              \frac{1}{2}(\mathcal{L}_1 + \mathcal{L}_2)\)
            </span>:
            <div style="display: flex;align-items: center;">
              <span class="math display" style="width:90%">\[\begin{split}
                \mathcal{L}_1 &amp; = \int_y \int_z q(y,z|x) \log
                \frac{p(x,y,z)}{q(z,y|x)}dzdy \\
                &amp; = \int_y \int_z q(z|x,y)q(y|x) \log
                \frac{p(x|y,z)p(z|y)p(y)}{q(z,y|x)}dzdy \\
                &amp; = \int_y q(y|x) \bigg(\int_z q(z|x,y) \log p(x|y,z) dz
                \\
                &amp; \quad + \int_z q(z|x,y) \log \frac{p(z|y)}{q(z|x,y)}
                dz \\
                &amp; \quad + \int_z q(z|x,y) \log \frac{p(y)}{q(y|x)} dz
                \bigg) dy\\
                &amp; = \int_y q(y|x) \bigg(\int_z q(z|x,y) \log p(x|y,z) dz
                \\
                &amp; \quad + \int_z q(z|x,y) \log \frac{p(z|y)}{q(z|x,y)}
                dz + \log \frac{p(y)}{q(y|x)} \bigg) dy \\
                &amp; = \int_y q(y|x) \bigg(\int_z q(z|x,y) \log p(x|y,z)
                dz\\
                &amp; \quad - D_{KL}(q(z|x,y)||p(z|y)) \bigg) dy \\
                &amp; \quad - D_{KL}(q(y|x)||p(y)) \\
                \end{split}\]
              </span>
              <div text-align=right>(4)</div>
            </div>

            We define <span class="math inline">\(\boldsymbol{\theta}\)</span> and <span
              class="math inline">\(\boldsymbol{\phi}\)</span> as parameters for the
            encoder <span class="math inline">\(q_{\boldsymbol{\phi}}(z|y,x)\)</span> and decoder
            network <span class="math inline">\(p_{\boldsymbol{\theta}}(x|y,z)\)</span>. We also
            define a network <span class="math inline">\(p_{\boldsymbol{\kappa}}(z|y)\)</span> with
            parameters <span class="math inline">\(\boldsymbol{\kappa}\)</span> for
            locating the base skills in the latent skill space. In our setups, the
            variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are
            conditionally
            independent given
            <span class="math inline">\(z\)</span>; the variables <span class="math inline">\(z\)</span> and <span
              class="math inline">\(y\)</span> are also conditionally independent
            given <span class="math inline">\(x\)</span>. Hence, the above equation
            can be simplified as:
            <div style="display: flex;align-items: center;">
              <span class="math display" style="width:90%">\[\begin{split}
                \mathcal{L}_1 &amp; = \int_y q(y|x) \bigg(\int_z
                q_{\boldsymbol{\phi}}(z|x) \log p_{\boldsymbol{\theta}}(x|z) dz \\
                &amp; \quad -
                D_{KL}(p_{\boldsymbol{\kappa}}(z|y)||q_{\boldsymbol{\phi}}(z|x)) \bigg)
                dy - D_{KL}(q(y|x)||p(y)) \\
                &amp; = \int_z q_{\boldsymbol{\phi}}(z|x) \log
                p_{\boldsymbol{\theta}}(x|z) dz - \int_y q(y|x)
                D_{KL}(q_{\boldsymbol{\phi}}(z|x)||p_{\boldsymbol{\kappa}}(z|y)) dy \\
                &amp; \quad - D_{KL}(q(y|x)||p(y)) \\
                &amp; = \mathbb{E}_{z \sim q_{\boldsymbol{\phi}}(z|x)} [\log
                p_{\boldsymbol{\theta}}(x|z)] - \int_y q(y|x)
                D_{KL}(q_{\boldsymbol{\phi}}(z|x)||p_{\boldsymbol{\kappa}}(z|y)) dy \\
                &amp; \quad - D_{KL}(q(y|x)||p(y)) \\
                \end{split}\]
              </span>
              <div text-align=right>(5)</div>
            </div>
            We know the variable
            <span class="math inline">\(y\)</span> is not continuous and has only
            three
            possibilities so it can be computed exactly by marginalizing over these
            three categorical options.
            <div style="display: flex;align-items: center;">
              <span class="math display" style="width:90%">\[\begin{split}
                \mathcal{L}_1 &amp; = \mathbb{E}_{z \sim
                q_{\boldsymbol{\phi}}(z|x)} [\log p_{\boldsymbol{\theta}}(x|z)] \\
                &amp; \quad - \sum_{k} q(y=k|x)
                D_{KL}(q_{\boldsymbol{\phi}}(z|x)||p_{\boldsymbol{\kappa}}(z|y=k)) \\
                &amp; \quad - D_{KL}(q(y|x)||p(y)) \\
                \end{split}\]
              </span>
              <div text-align=right>(6)</div>
            </div>
            In terms of <span class="math inline">\(\mathcal{L}_2\)</span>, we have <span
              class="math display">\[\mathcal{L}_2 = \mathbb{E}_{z \sim
              q_{\boldsymbol{\phi}}(z|x)}[\log p_{\boldsymbol{\theta}}(x|z)] - D_{KL}
              (q_{\boldsymbol{\phi}}(z|x)||p(z))\]</span> Then, the total <span
              class="math inline">\(\mathcal{L}_{\textrm{ELBO}}\)</span> is formalized
            as:
            <div style="display: flex;align-items: center;">
              <span class="math display" style="width:90%">\[\begin{split}
                \mathcal{L}_{\textrm{ELBO}} &amp; = \overbrace{\mathbb{E}_{z \sim
                q_{\boldsymbol{\phi}}(z|x)}[\log
                p_{\boldsymbol{\theta}}(x|z)]}^{\textrm{reconstruction loss}} - \beta_1
                \overbrace{D_{KL}
                (q_{\boldsymbol{\phi}}(z|x)||p(z))}^{\textrm{regularizer
                ($\mathcal{L}_{\textrm{reg.}}$)}}\\
                &amp; - \beta_2 \sum_{k} q(y=k|x)
                \underbrace{D_{KL}(q_{\boldsymbol{\phi}}(z|x)||p_{\boldsymbol{\kappa}}(z|y=k))}_{\textrm{base-skill
                regularizer ($\mathcal{L}_\textrm{skill}$)}}
                \end{split}\text{,}\]
              </span>
              <div text-align=right>(7)</div>
            </div>
            </p>
            <h3 id="appedixsec:Imitation Learning with Base Skill Priors">Imitation
              Learning with Base Skill Priors</h3>
            <p>The objective of our model is to learn a policy <span class="math inline">\(\pi(x|s_c,s_g)\)</span>
              conditioned on
              the current
              state <span class="math inline">\(s_c\)</span> and the goal state <span class="math inline">\(s_g\)</span>
              and
              outputting <span class="math inline">\(x\)</span>, a sequence of actions, namely a skill.
              Since we introduced the base skill concept into our model, the policy
              <span class="math inline">\(\pi(\cdot)\)</span> should also find the
              best base skill <span class="math inline">\(y\)</span> for the current
              observation. We have <span class="math inline">\(\pi(x,y|s_c,s_g)\)</span>, where <span
                class="math inline">\(y\)</span> is the base skill the agent chooses
              based on the current state and goal state.<br />
              Inspired by the conditional variational autoencoder (CVAE):
            <div style="display: flex;align-items: center;">
              <span class="math display" style="width:90%">\[\log p(x|c) \ge
                \mathbb{E}_{q(z|x,c)}[\log
                p(x|z,c)] - D_{KL}(q(z|x,c) || p(z|c))\]
              </span>
              <div text-align=right>(8)</div>
            </div>
            where c is a symbol to
            describe a general condition, we would like to extend the above equation
            by integrating <span class="math inline">\(y\)</span> which indicates
            the base skill. The evidence we would like to maximize then turns to
            <span class="math inline">\(p(x,y|c)\)</span>. We employ the approximate
            variational posterior <span class="math inline">\(q(y,z|x,c)\)</span> to
            approximate the intractable true posterior <span class="math inline">\(p(y,z|x,c)\)</span> where <span
              class="math inline">\(z\)</span> indicates the skill embeddings in the
            skill latent space. We intend to find the ELBO by measuring the KL
            divergence between the true posterior and the posterior
            approximation.
            </p>
            <p>
            <div style="display: flex;align-items: center;">
              <span class="math display" style="width:90%">\[% \resizebox{0.5\textwidth}{!}{%
                % $
                \begin{split}
                &amp;D_{KL}(q(y,z|x,c) || p(y,z|x,c)) \\
                = &amp; \int_y \int_z q(y,z|x,c) \log
                \frac{q(y,z|x,c)}{p(y,z|x,c)}dzdy \\
                = &amp; - \int_y \int_z q(y,z|x,c) \log
                \frac{p(y,z|x,c)}{q(y,z|x,c)}dzdy \\
                = &amp; - \int_y \int_z q(y,z|x,c) \log
                \frac{p(x,y,z|c)}{q(z,y|x,c)}dzdy + \log p(x|c) \\
                \end{split}
                % $%
                % }\]
              </span>
              <div text-align=right>(9)</div>
            </div>
            </p>
            <p>We focus on the ELBO term:
            <div style="display: flex;align-items: center;">
              <span class="math display" style="width:90%">\[\begin{split}
                \mathcal{L} &amp; = \int_y \int_z q(y,z|x,c) \log
                \frac{p(x,y,z|c)}{q(z,y|x,c)}dzdy \\
                &amp; = \int_y \int_z q(z|x,y,c)q(y|x,c) \log
                \frac{p(x|y,z,c)p(z|y,c)p(y|c)}{q(z,y|x,c)} dz dy \\
                &amp; = \int_y q(y|x,c)\bigg(\int_z q(z|x,y,c) \log
                p(x|y,z,c) dz \\
                &amp; \quad + \int_z q(z|x,y,c) \log
                \frac{p(z|y,c)}{q(z|x,c)} dy + \log \frac{p(y|c)}{q(y|x,c)}\bigg) \\
                &amp; = \int_y q(y|x,c) \bigg( \int_z q(z|x,y,c)
                \log p(x|y,z,c) dz \\
                &amp; \quad - D_{KL} (q(z|x,y,c)||p(z|y,c)) \bigg)
                dy - D_{KL} (q(y|x,c)||p(y))
                \end{split}\]
              </span>
              <div text-align=right>(10)</div>
            </div>
            To have a clear explanation, we examine each
            element in the equation mentioned above.</p>
            <ul>
              <li>
                <p>variable <span class="math inline">\(x,y,z,c\)</span>:</p>
                <ul>
                  <li>
                    <p><span class="math inline">\(x\)</span>: action sequence (skill)
                      the agent chooses</p>
                  </li>
                  <li>
                    <p><span class="math inline">\(y\)</span>: base skill
                      priors</p>
                  </li>
                  <li>
                    <p><span class="math inline">\(z\)</span>: skill embeddings in the
                      latent skill space</p>
                  </li>
                  <li>
                    <p><span class="math inline">\(c\)</span>: a combination of the
                      current state and the goal state <span class="math inline">\((s_c,s_g)\)</span></p>
                  </li>
                </ul>
              </li>
              <li>
                <p><span class="math inline">\(q(y|x,c)\)</span>: It corresponds to
                  the base skill labeler part in our SPIL model. We define this network
                  with parameters <span class="math inline">\(\boldsymbol{\omega}\)</span>
                  and simplify it as <span class="math inline">\(q_{\boldsymbol{\omega}}(y|c)\)</span> by pointing
                  out <span class="math inline">\(x\)</span>.</p>
              </li>
              <li>
                <p><span class="math inline">\(q(z|x,y,c)\)</span>: It refers to the
                  encoder network <span class="math inline">\(f_{\boldsymbol{\Phi}}\)</span> plus the skill
                  embedding selector network <span class="math inline">\(f_{\boldsymbol{\lambda}}\)</span>, taking <span
                    class="math inline">\(c\)</span> as input in our settings. It can be
                  written as <span class="math inline">\(q_{\boldsymbol{\Phi},
                    \boldsymbol{\lambda}}(z|c)\)</span>, pointing out <span class="math inline">\(x,y\)</span>.</p>
              </li>
              <li>
                <p><span class="math inline">\(p(x|y,z,c)\)</span>: It is the skill
                  generator network <span class="math inline">\(f_{\boldsymbol{\theta}}\)</span> with parameters
                  <span class="math inline">\(\boldsymbol{\theta}\)</span>. This network
                  only takes <span class="math inline">\(z,c\)</span> as input in our
                  setting and we consider <span class="math inline">\(x\)</span> and <span
                    class="math inline">\(y\)</span> to be
                  conditionally independent given
                  <span class="math inline">\(z,c\)</span>. It can then be formalized as
                  <span class="math inline">\(p(x|z,c)\)</span>. Note that the parameters
                  of this network are pretrained and frozen during the training
                  process.
                </p>
              </li>
              <li>
                <p><span class="math inline">\(p(z|y,c)\)</span>: It is the base
                  skill prior locater <span class="math inline">\(f_{\boldsymbol{\kappa}}\)</span> with parameter
                  <span class="math inline">\(\boldsymbol{\kappa}\)</span>. We assume
                  <span class="math inline">\(z\)</span> and <span class="math inline">\(c\)</span> are conditionally
                  independent
                  given
                  <span class="math inline">\(y\)</span> so that we have <span
                    class="math inline">\(p_{\boldsymbol{\kappa}}(z|y)\)</span>. Note that
                  the parameters of this network are frozen during the training.
                </p>
              </li>
              <li>
                <p><span class="math inline">\(p(y)\)</span>: It is the prior
                  distribution for base skills <span class="math inline">\(y\)</span>
                  which is drawn from a categorical distribution.</p>
              </li>
            </ul>
            <p>Based on the above analysis, the whole equation can be simplified as
              follows:
            <div style="display: flex;align-items: center;">
              <span class="math display" style="width:90%">\[\begin{split}
                \mathcal{L} &amp; =\int_y q_{\boldsymbol{\omega}}(y|c) \bigg( \int_z
                q_{\boldsymbol{\Phi}, \boldsymbol{\lambda}}(z|c) \log
                p_{\boldsymbol{\theta}}(x|z,c) dz \\
                &amp; \quad - D_{KL}(q_{\boldsymbol{\Phi},
                \boldsymbol{\lambda}}(z|c)||p_{\boldsymbol{\kappa}}(z|y))\bigg) dy -
                D_{KL}(q(y|c)||p(y)) \\
                &amp; = \int_z q_{\boldsymbol{\Phi},
                \boldsymbol{\lambda}}(z|c) \log p_{\boldsymbol{\theta}}(x|z,c) dz \\
                &amp; \quad - \int_y q_{\boldsymbol{\omega}}(y|c)
                D_{KL}(q_{\boldsymbol{\Phi},
                \boldsymbol{\lambda}}(z|x,c)||p_{\boldsymbol{\kappa}}(z|y)) dy \\
                &amp; \quad - D_{KL}(q_{\boldsymbol{\omega}}(y|c)||p(y))
                \\
                &amp; = \mathbb{E}_{z \sim q_{\boldsymbol{\Phi},
                \boldsymbol{\lambda}}(z|c)}\log p_{\boldsymbol{\theta}}(x|z,c) \\
                &amp; \quad - \sum_{k} q_{\boldsymbol{\omega}}(y=k|c)
                D_{KL}(q_{\boldsymbol{\Phi},
                \boldsymbol{\lambda}}(z|c)||p_{\boldsymbol{\kappa}}(z|y)) \\
                &amp; \quad - D_{KL}(q_{\boldsymbol{\omega}}(y|c)||p(y))
                \end{split}\]
              </span>
              <div text-align=right>(11)</div>
            </div>
            By introducing two weights <span class="math inline">\(\gamma_1\)</span> and <span
              class="math inline">\(\gamma_2\)</span> for the regularization terms, we
            have
            <div style="display: flex;align-items: center;">
              <span class="math display" style="width:90%">\[\begin{split}
                \mathcal{L} &amp; = \overbrace{\mathbb{E}_{z \sim
                q_{\boldsymbol{\Phi}, \boldsymbol{\lambda}}(x,c)}\log
                p_{\boldsymbol{\theta}}(x|z,c)}^{\textrm{Reconstruction loss}} \\
                &amp; - \gamma_1 \sum_{k} q_{\boldsymbol{\omega}}(y=k|c)
                \overbrace{D_{KL}(q_{\boldsymbol{\Phi},
                \boldsymbol{\lambda}}(z|x,c)||p_{\boldsymbol{\kappa}}(z|y))}^{\textrm{Base
                skill regularizer ($\mathcal{L}_{\textrm{skill}}$)}}\\
                &amp; - \gamma_2
                \overbrace{D_{KL}(q_{\boldsymbol{\omega}}(y|c)||p(y))}^{\textrm{Categorical
                regularizer ($\mathcal{L}_{\textrm{cat.}}$)}}
                \end{split}\]
              </span>
              <div text-align=right>(12)</div>
            </div>
            Here, we use Huber loss as the metric for
            reconstructive loss. Intuitively, the base skill regularizer is used to
            regularize a skill embedding, depending on its base skill categorial.
            The categorial regularizer aims to regularize the base skill
            classification based on the prior categorical distribution of <span class="math inline">\(y\)</span>.
            <h2>Reference</h2>
            <p id="ref:calvin">[1] Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks,” IEEE Robotics and Automation Letters (RA-L)</p>
            <p id="ref:SpiRL">[2] L. X. Shi, J. J. Lim, and Y. Lee, “Skill-based model-based reinforcement learning,” in 6th Annual Conference on Robot Learning, 2022.</p>
            <p id="ref:SkiMo">[3] K. Pertsch, Y. Lee, and J. J. Lim, “Accelerating reinforcement learning with learned skill priors,” in Conference on Robot Learning (CoRL), 2020. </p>
          </div>
        </div>
  </section>

  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-desktop content">
        <h2 class="title is-3">Videos</h2>
        <div class="columns is-vcentered">
          <div class="column is-5">
            <div class="column is-centered has-text-centered">
              <video poster="" id="video1" width="400" height="400" controls>
                <!-- Your video file here -->
                <source src="static/videos/_long_horizon_sequence_success.mp4" type="video/mp4">
              </video>
            </div>

          </div>
          <div class="column">
            <div class="column is-centered has-text-centered">
              <video poster="" id="video2" width="712" height="400" controls>
                <!-- Your video file here -->
                <source src="static/videos/Sequential%20Task%20Completion.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>




  <!-- Paper poster -->
  <!--<section class="hero is-small is-light">-->
  <!--  <div class="hero-body">-->
  <!--    <div class="container">-->
  <!--      <h2 class="title">Poster</h2>-->

  <!--      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">-->
  <!--          </iframe>-->
  <!--        -->
  <!--      </div>-->
  <!--    </div>-->
  <!--  </section>-->
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @misc{zhou2023languageconditioned,
              title={Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data},
              author={Hongkuan Zhou and Zhenshan Bing and Xiangtong Yao and
              Xiaojie Su and Chenguang Yang and Kai Huang and Alois Knoll},
              year={2023},
              eprint={2305.19075},
              archivePrefix={arXiv},
              primaryClass={cs.RO}
        }
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->

  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-desktop content">
        <div class="columns is-vcentered">
          <div class="column">
            <div class="column is-centered has-text-centered">
              <span class="icon">
                <i class="fa fa-map-marker"></i>
              </span>
              <div class="content">
                <h4>Address</h4>
                <p>Boltzmannstr. 3, 85748 Garching, Germany</p>
              </div>
            </div>

          </div>
          <div class="column">
            <div class="column is-centered has-text-centered">
              <span class="icon">
                <i class="fas fa-envelope"></i>
              </span>
              <div class="content">
                <h4>Contact</h4>
                <a href="mailto:zhenshan.bing@tum.de">zhenshan.bing@tum.de</a>
              </div>
            </div>
          </div>
        </div>
        <div>
        </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This work is supported by Lehrstuhl für Robotik, Künstliche Intelligenz und Echtzeitsysteme TUM School of
              Computation, Information and Technology Technische Universität München.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>


  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
